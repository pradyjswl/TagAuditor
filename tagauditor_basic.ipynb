#Python 3.8.5

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from tqdm import tqdm #shows progress bar
from csv import writer,reader,DictWriter
import pyfiglet #It takes ASCII text and renders it in ASCII art fonts
from IPython.display import display, HTML
import sys

start_page = 'https://chatrwireless.com/web/content/data_services#update-phone-settings-to-use-data'
limit = 500

def is_valid_url(url):
    import re
    regex = re.compile(
        r'^https?://'  # http:// or https://
        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
        r'localhost|'  # localhost...
        r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})' # ...or ip
        r'(?::\d+)?'  # optional port
        r'(?:/?|[/?]\S+)$', re.IGNORECASE)
    return url is not None and regex.search(url)


def get_protocol(url):        
    if ('https:' in url):
        p = 'https://'
    elif ('http:' in url):
        p = 'http://'
    else:
        p = 'http://'
    return p


def formatted_filename():
    t1 = time.asctime(time.localtime(time.time()))
    x = t1.split(" ")
    if x[2] != "":
        time_variable = x[3].split(":")
        time_variable = time_variable[0]+"-"+time_variable[1]+"-"+time_variable[2]
        new_format = x[2]+"_"+x[1]+"_"+x[4]+"_"+x[0]+"_("+time_variable+")"
    else:
        time_variable = x[4].split(":")
        time_variable = time_variable[0]+"-"+time_variable[1]+"-"+time_variable[2]
        new_format = x[3]+"_"+x[1]+"_"+x[5]+"_"+x[0]+"_("+time_variable+")"               
    return "tms_scanner_"+str(new_format)+".csv"


def _depthChecker(_parentPage):
    if (_parentPage == 'start_page'):
        return 1
    else:
        return df.loc[df['Page_URL'] == _parentPage, 'Depth'].iloc[0]+1

    
def _scodeChecker(_arr):
    if (len(_arr) > 0):
        for i, item in enumerate(_arr):
            str_item = str(item)
            if (legacy_snippet in str_item):         
                return item.attrs['src']
            elif (i == (len(_arr)-1)):
                return "Scode Not Found"
    else:
        return 'No <script> tags on page'
    
    
def _gtmChecker(_arr):
    if (len(_arr) > 0):
        for i, item in enumerate(_arr):
            str_item = str(item)
            if (gtm_snippet in str_item):         
                split_str_item = str_item.split(';')
                for val in split_str_item:
                    if ("GTM-" in val):
                        container_code = (val.split(',')[4]).replace("')",'').replace('\'','')
                        return (gtm_snippet+container_code)          
            elif (i == (len(_arr)-1)):
                return "GTM Not Found"
    else:
        return 'No <script> tags on page'

    
def _dtmChecker(_arr):
    if (len(_arr) > 0):
        for i, item in enumerate(_arr):
            str_item = str(item)
            if (adobe_snippet in str_item and 'launch' not in str_item):         
                return item.attrs['src']
            elif (i == (len(_arr)-1)):
                return "DTM Not Found"
    else:
        return 'No <script> tags on page'

    
def _launchChecker(_arr):
    if (len(_arr) > 0):
        for i, item in enumerate(_arr):
            str_item = str(item)
            if (adobe_snippet in str_item and 'launch' in str_item):         
                return item.attrs['src']
            elif (i == (len(_arr)-1)):
                return "Launch Not Found"
    else:
        return 'No <script> tags on page'

    
def _indexer(_arr,current_domain):
    current_page=current_domain
    protocol = get_protocol(current_domain)
    current_domain = current_domain.replace('http://','').replace('https://','').replace('www.','')
    current_domain = current_domain.split('/')[0]
    if (len(_arr) > 0):
        print("Page:",current_page)
        for val in _arr:
            try:           
                _href = (val.attrs['href'])
                if ((base_domain in _href) and (_href not in str(master_hrefs)) and (_href not in exclude_list)):
                    _obj = {}
                    _obj['page_url'] = _href
                    _obj['crawlStatus'] = 'pending'
                    _obj['Parent_Page_URL'] = current_page
                    print('Pending:',_obj['page_url'])
                    master_hrefs.append(_obj) 
                elif (_href[0] == '/' and (str(protocol) + (current_domain + _href).replace('//','/') not in str(master_hrefs)) and (_href not in exclude_list)):
                    _obj = {}
                    _obj['page_url'] = str(protocol) + (current_domain + _href).replace('//','/')
                    _obj['crawlStatus'] = 'pending'
                    _obj['Parent_Page_URL'] = current_page
                    print('Pending:',_obj['page_url'])
                    master_hrefs.append(_obj)
            except:
                pass
    

def _crawler(url, parentPageUrl):
    try:
        r = requests.get(url)
        statusCode = r.status_code
        
        if (statusCode == (200 or 400)):
            content = r.content
            soup = BeautifulSoup(content, 'lxml')
            allScriptsList = soup.find_all('script')
            allAnchorTags = soup.find_all('a')
            pageTitle = soup.find('title')
            pageTitle = pageTitle.string
            #indexer function will be called here on allAnchorTags
            _indexer(allAnchorTags,str(url))
            #format file
            outputArr = []
            outputArr.append(len(df)+1)
            outputArr.append(url)
            outputArr.append(statusCode)
            outputArr.append(pageTitle)
            outputArr.append(_depthChecker(parentPageUrl))         
            outputArr.append(parentPageUrl)         
            outputArr.append(_gtmChecker(allScriptsList))
            outputArr.append(_scodeChecker(allScriptsList))
            outputArr.append(_dtmChecker(allScriptsList))
            outputArr.append(_launchChecker(allScriptsList))
            
            #print(outputArr)
            df_length = len(df)
            df.loc[df_length] = outputArr
            #display(HTML(df.to_html(index=False)))             #to display ALL records each time
            
            df1.loc[0] = outputArr                            #to display only one record at a time
            display(HTML(df1.to_html(header=False, index=False)))
    except Exception as e: 
        print('Unexpected error occured during crawl for below url, Please retart program \n',url,'\n',e)
        quit()   
        

def _initiator(limit):
    #for _index, _val in enumerate(tqdm(master_hrefs)): #shows progress bar
    display(HTML(df1.to_html(header=True)))
    for _index, _val in enumerate(master_hrefs):    
        if (_index < int(limit)):
            if (_val['crawlStatus'] == 'pending'):
                time.sleep(5)
                _crawler(_val['page_url'], _val['Parent_Page_URL'])
                _val['crawlStatus'] = 'done'
        else:
            return
        
        
f = pyfiglet.Figlet(font='slant')
print(f.renderText('TMS Scanner'))

# Get Values form User
#start_page = str(input('\nPlease provide start_page/homepage of your site in format http://bell.ca or https://www.chatrwireless.com or https://westjet.com/en-ca \n>> '))
start_page = start_page.strip(' ')
    
if (is_valid_url(start_page) == None):
    print('\nIncorrect Format for Start Page > Please Restart Program\n')
    sys.exit('Prgoram Exit') #quit/exit works when python executed from command line
    
#limit = (input('\nDefault Scan Limit is set to 100 pages: \nSet custom scan limit (hit enter for no change) \n>> '))
print('\nScanner Started..\n\n')

if (limit != '' and int(limit) > 0 and int(limit) <= 500):
    limit = limit
else:
    limit = 100
    
#base domain can come form input or api call
base_domain = start_page.replace('http://','').replace('https://','').replace('www.','')
exclude_list = ['mailto:','javascript:','#']

gtm_snippet = 'www.googletagmanager.com/gtm.js?id='
adobe_snippet = 'assets.adobedtm.com'
legacy_snippet = 's_code'
master_hrefs = ['https://chatrwireless.com/web/chatr.portal?_nfpb=true&_pageLabel=HomeLanding']

#suffix the url with forward slash
if (start_page[len(start_page)-1] == "/"):
    pass
else:
    start_page = str(start_page) + '/'
    
_obj = {}
_obj['page_url'] = start_page
_obj['crawlStatus'] = 'pending'
_obj['Parent_Page_URL'] = 'start_page'

master_hrefs.append(_obj)

column_names = ["Sr_No", "Page_URL", "Status_Code", "Page_Title", "Depth", "Parent_Page_URL", "GTM_Snippet", "Legacy_Scode_snippet", "DTM_Snippet", 'Launch_Snippet']
df = pd.DataFrame(columns = column_names)
df1 = pd.DataFrame(columns = column_names)

_initiator(limit)
filename = formatted_filename()
df.to_csv(filename, index=False, encoding='utf-8')

print('\n\n')
print(f"Program Complete\nPlease refer file: {filename} for reference.")              
print('\n')
